{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19068cdf",
   "metadata": {},
   "source": [
    "# PyTorch → ONNX → TensorRT\n",
    "\n",
    "Simple conversion pipeline for ResNet50:\n",
    "- Load PyTorch model\n",
    "- Export to ONNX\n",
    "- Build TensorRT engine\n",
    "- Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04401665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 24 02:18:12 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060        Off |   00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   43C    P8             19W /  170W |     647MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            4290      G   /usr/lib/xorg/Xorg                      127MiB |\n",
      "|    0   N/A  N/A            4592      G   /usr/bin/gnome-shell                      8MiB |\n",
      "|    0   N/A  N/A            5114      G   ...exec/xdg-desktop-portal-gnome          2MiB |\n",
      "|    0   N/A  N/A           39940      G   /usr/share/code/code                     34MiB |\n",
      "|    0   N/A  N/A         1740744      G   .../7559/usr/lib/firefox/firefox         18MiB |\n",
      "|    0   N/A  N/A         1746638      G   /usr/share/code/code                     46MiB |\n",
      "|    0   N/A  N/A         3901779      G   /usr/bin/nautilus                        19MiB |\n",
      "|    0   N/A  N/A         3903983      G   .../7559/usr/lib/firefox/firefox          5MiB |\n",
      "|    0   N/A  N/A         3915174      G   /usr/lib/xorg/Xorg                      198MiB |\n",
      "|    0   N/A  N/A         3915499      G   /usr/bin/gnome-shell                      6MiB |\n",
      "|    0   N/A  N/A         3916015      G   /usr/libexec/gnome-initial-setup         11MiB |\n",
      "|    0   N/A  N/A         3916640      G   /usr/bin/nautilus                        16MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53228b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Tue_Dec_16_07:23:41_PM_PST_2025\n",
      "Cuda compilation tools, release 13.1, V13.1.115\n",
      "Build cuda_13.1.r13.1/compiler.37061995_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1d01b",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96c434c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu130\n",
    "!pip install -q tensorrt==10.8.0.43 --extra-index-url https://pypi.nvidia.com\n",
    "!pip install -q onnx pycuda numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27434178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.1+cu118\n",
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import onnx\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "ONNX_PATH = 'models/resnet50.onnx'\n",
    "TENSORRT_PATH = 'models/resnet50.engine'\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5239f",
   "metadata": {},
   "source": [
    "## 2. Load Model & Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb4090e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded ResNet50\n"
     ]
    }
   ],
   "source": [
    "# Load PyTorch ResNet50\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "model.eval()\n",
    "print(f'✅ Loaded ResNet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49bef52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved ONNX: models/resnet50.onnx\n"
     ]
    }
   ],
   "source": [
    "# Convert to ONNX\n",
    "dummy_input = torch.randn(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE)\n",
    "torch.onnx.export(\n",
    "    model.cpu(),\n",
    "    dummy_input,\n",
    "    ONNX_PATH,\n",
    "    opset_version=18,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch'}, 'output': {0: 'batch'}}\n",
    ")\n",
    "\n",
    "onnx.checker.check_model(onnx.load(ONNX_PATH))\n",
    "print(f'✅ Saved ONNX: {ONNX_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d8d517",
   "metadata": {},
   "source": [
    "## 3. Convert to TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c736176",
   "metadata": {},
   "source": [
    "## 3.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73be072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "# Define Logger\n",
    "logger = trt.Logger(trt.Logger.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6293ba35",
   "metadata": {},
   "source": [
    "## 3.2 Define builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec9eae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = trt.Builder(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06351c6e",
   "metadata": {},
   "source": [
    "## 3.3 Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80b353a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc9772b",
   "metadata": {},
   "source": [
    "## 3.4 Parse ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70e1c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = trt.OnnxParser(network, logger)\n",
    "\n",
    "# Parse ONNX\n",
    "with open(ONNX_PATH, 'rb') as f:\n",
    "    parser.parse(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5863b",
   "metadata": {},
   "source": [
    "## 3.5 Configure engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c29ec96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = builder.create_builder_config()\n",
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d3275",
   "metadata": {},
   "source": [
    "## 3.6 Dynamic batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1b457a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set dynamic batch profile\n",
    "profile = builder.create_optimization_profile()\n",
    "profile.set_shape('input', (1, 3, IMG_SIZE, IMG_SIZE), (1, 3, IMG_SIZE, IMG_SIZE), (8, 3, IMG_SIZE, IMG_SIZE))\n",
    "config.add_optimization_profile(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1cdffb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FP16 enabled\n",
      "Building engine (may take a few minutes)...\n",
      "✅ Saved TensorRT engine: models/resnet50.engine\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Enable FP16 if available\n",
    "if builder.platform_has_fast_fp16:\n",
    "    config.set_flag(trt.BuilderFlag.FP16)\n",
    "    print('✅ FP16 enabled')\n",
    "\n",
    "# Build and save\n",
    "print('Building engine (may take a few minutes)...')\n",
    "engine = builder.build_serialized_network(network, config)\n",
    "with open(TENSORRT_PATH, 'wb') as f:\n",
    "    f.write(engine)\n",
    "\n",
    "print(f'✅ Saved TensorRT engine: {TENSORRT_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c8cbb",
   "metadata": {},
   "source": [
    "## 4. Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0bb64d",
   "metadata": {},
   "source": [
    "### 4.1 Init CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "453beb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10508602",
   "metadata": {},
   "source": [
    "### 4.2 Load engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TENSORRT_PATH, 'rb') as f:\n",
    "    runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))\n",
    "    engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05ce1109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor name: input\n",
      "Output tensor name: output\n"
     ]
    }
   ],
   "source": [
    "# Get tensor names\n",
    "input_name = engine.get_tensor_name(0)\n",
    "output_name = engine.get_tensor_name(1)\n",
    "\n",
    "print(f'Input tensor name: {input_name}')\n",
    "print(f'Output tensor name: {output_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98e0b39",
   "metadata": {},
   "source": [
    "### 4.3  Set input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a6063fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE)\n",
    "print(f'Input shape: {input_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab329dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.set_input_shape(input_name, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d26a2ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (1, 1000)\n"
     ]
    }
   ],
   "source": [
    "output_shape = context.get_tensor_shape(output_name)\n",
    "\n",
    "print(f'Output shape: {output_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbce722",
   "metadata": {},
   "source": [
    "### 4.4 Allocate memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a0eb012",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_input = np.random.randn(*input_shape).astype(np.float32)\n",
    "h_output = np.empty(output_shape, dtype=np.float32)\n",
    "d_input = cuda.mem_alloc(h_input.nbytes)\n",
    "d_output = cuda.mem_alloc(h_output.nbytes)\n",
    "stream = cuda.Stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77688663",
   "metadata": {},
   "source": [
    "### 4.5 Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77cbd8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set tensor addresses\n",
    "context.set_tensor_address(input_name, int(d_input))\n",
    "context.set_tensor_address(output_name, int(d_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45ba1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.memcpy_htod_async(d_input, h_input, stream)\n",
    "context.execute_async_v3(stream_handle=stream.handle)\n",
    "cuda.memcpy_dtoh_async(h_output, d_output, stream)\n",
    "stream.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e6aae20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inference completed\n",
      "Top-5 classes: [904 490 828 556 488]\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "print(f'✅ Inference completed')\n",
    "print(f'Top-5 classes: {np.argsort(h_output[0])[-5:][::-1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnx_tensorrt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
